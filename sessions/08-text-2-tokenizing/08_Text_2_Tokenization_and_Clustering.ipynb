{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08 - Text 2 - Tokenization and Clustering",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMh0MhDP5lG0"
      },
      "source": [
        "# LEARNING GOALS\n",
        "#\n",
        "#                 - tokenization deeper dive\n",
        "#                 - introduce ML integrations\n",
        "#                 - reinforce text prep\n",
        "#                 - Cluster documents\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em6KP7t66Vpn"
      },
      "source": [
        "# installs\n",
        "! pip install newspaper3k\n",
        "! pip install spacy\n",
        "! pip install wordcloud\n",
        "! pip install emoji\n",
        "! pip install nltk\n",
        "! pip install scikit-plot\n",
        "! pip install umap-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHt-hLKZ6YJH"
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scikitplot as skplot\n",
        "\n",
        "# some \"fun\" packages\n",
        "from wordcloud import WordCloud\n",
        "import emoji\n",
        "\n",
        "import re\n",
        "\n",
        "# new imports\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer  \n",
        "import nltk\n",
        "\n",
        "from newspaper import Article\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-K8BsBw6iq6"
      },
      "source": [
        "# lets go back to our simple dataset\n",
        "a = ['I like turtles!',\n",
        "     'You like hockey and golf ',\n",
        "     'Turtles and hockey ftw',\n",
        "     'Python is very easy to learn. ðŸ',\n",
        "     'A great resource is www.spacy.io',\n",
        "     ' Today is the Feb 22, 2021 !           ',\n",
        "     '@username #hashtag https://www.text.com',\n",
        "     'BA820 ',\n",
        "     'My name is Brock and my phone number is 617-867-5309']\n",
        "\n",
        "df = pd.DataFrame({'text':a})\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWEhJvoc6yMB"
      },
      "source": [
        "# remember, the printout is cleaned up!\n",
        "df.text.values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po113nuF6505"
      },
      "source": [
        "# what we saw previously was the intuition of tokenzing our data\n",
        "# but its not hard to imagine that there has been a great deal of work towards this task\n",
        "# let's start with just getting the \n",
        "\n",
        "# first, lets use sklearn\n",
        "cv = CountVectorizer()\n",
        "tokens = cv.fit_transform(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djgza9Bp_sY6"
      },
      "source": [
        "# what do we have?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0fVtT6d_y-C"
      },
      "source": [
        "# shape?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRZ6HXpO_2gH"
      },
      "source": [
        "# we get a sparse array which is nice, but we can \"expand\" this\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXAiW9OrBpez"
      },
      "source": [
        "# confirm the length matches\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETXU4NC__8mJ"
      },
      "source": [
        "# this is a bag of words approach\n",
        "# one row for the document, and a count vector for each token\n",
        "#\n",
        "# NOTE:  sklearn is tokenizing the words for us, but we will come back to this\n",
        "\n",
        "# what's nice about sklearn is that it keeps things simple and retains\n",
        "# the nice aspects of working in this toolkit\n",
        "\n",
        "# get the features and the index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuURmm_6PdBr"
      },
      "source": [
        "# we can also extract the feature names\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TOQxpUOB-18"
      },
      "source": [
        "## when considering the original input, what stands out to you?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvVv_ELlCQjb"
      },
      "source": [
        "# you can always just pull out the feature names if you want\n",
        "# but the goal here is that sklearn is keeping this to help with downstream ml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13t1J3a_CQpQ"
      },
      "source": [
        "############################################ your turn\n",
        "## URL = https://voicebot.ai/2021/02/16/conversational-ai-startup-admithub-raises-14m-for-higher-ed-chatbots/\n",
        "## get the page and extract the text  (HINT: newspaper or requests/beautifulsoup can help!)\n",
        "## tokenize the page (CountVectorizer)\n",
        "## how many tokens do you have?\n",
        "## TRICKY!  which word appears the most often?  what is it's index?\n",
        "## TIP: may need to pass to sklearn as a list of length 1\n",
        "\n",
        "## REMEMBER: a few ways to do this, but always can copy/paste if you don't want to scrape with newspaper or requests/soup\n",
        "\n",
        "\n",
        "URL = \"https://voicebot.ai/2021/02/16/conversational-ai-startup-admithub-raises-14m-for-higher-ed-chatbots/\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9tUz8TeFvO5"
      },
      "source": [
        "################################################## Lets summarize so far\n",
        "##\n",
        "## we can use sklearn to keep things in our typical ml format\n",
        "## we can see that there is some pre-processing taking place\n",
        "## lets dive into that a bit more, and then discuss a flow using nltk -> sklearn\n",
        "\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "\n",
        "# - Notice the lower casing by default\n",
        "# - we can pass our own regex/tokenizer if we wanted, and some people do this (build their own)\n",
        "# - different ways to tokenize\n",
        "# - there are stopwords, but we can pass anything\n",
        "# - we can set the max number of tokens\n",
        "# - we can one hot encode = instead of counts, it can be 0/1 for the word/token\n",
        "# - we can create ngrams\n",
        "# - we can even validate the vocabulary if we wanted\n",
        "#\n",
        "# This last point brings up the concept of unseen words\n",
        "# Remember! sklearn fits the object, so any unseen words will not be parsed on new datsets with transform\n",
        "#\n",
        "# Summary: really powerful and adaptable, but means you plug in your own regex/tools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvXesJ93G-3S"
      },
      "source": [
        "################### part 1: - lets start with ngrams\n",
        "##\n",
        "## instead of single tokens, we can try to capture context by windowing the tokens/phrases\n",
        "## we can pass in a tuple of the ngrams, default is 1,1\n",
        "\n",
        "# a new dataset\n",
        "corpus = [\"tokens, tokens everywhere\"]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrtFud-R_fV5"
      },
      "source": [
        "# we could only have bigrams\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91NcQptP_O2x"
      },
      "source": [
        "# the key point is that you can imagine it might be able to retain context\n",
        "# if we combine tokens with other n-grams.  \n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdr2JnoGAjGW"
      },
      "source": [
        "###################################### Quick task\n",
        "## \n",
        "## build off the article from above\n",
        "## but instead of parsing the tokens (unigrams), include bigrams (2) and trigrams (3) \n",
        "## to the feature space\n",
        "##\n",
        "## how many features have we extracted from the article?\n",
        "## \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCovWMZ1KtnM"
      },
      "source": [
        "###################################### Question\n",
        "###### what does this say about our choice of tokenization\n",
        "###### what tools might help with this \"issue\"?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jmwz8YzLDn9"
      },
      "source": [
        "###################################### Stopwords\n",
        "## by default stop words are not removed\n",
        "## there is a pre-built list of words, but let's ignore it\n",
        "## nltk is a great toolkit, and we will explore it later, but for now\n",
        "## lets just use the stopwords from that package\n",
        "\n",
        "# if this is your first time, you may need to download the stopwords\n",
        "# or on colab, for your session\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "## OF COURSE, you could always downlod your own.  not the format of below, we just pass in a list in the end\n",
        "\n",
        "# lets get the stopwords\n",
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = list(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPTEjJ9BLQ2_"
      },
      "source": [
        "# what do we have?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSyzS47KNEcX"
      },
      "source": [
        "# the first few\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTbTa_p0NGpA"
      },
      "source": [
        "# note that everything is lower case!\n",
        "\n",
        "# import random\n",
        "# random.choices(STOPWORDS, k=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3DxF4LTNKUh"
      },
      "source": [
        "# admittedly this is harder to find than it should be\n",
        "# but the languages supported in NLTK\n",
        "stopwords.fileids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTlx-eqLN1x8"
      },
      "source": [
        "# now you can imagine that is pretty limiting above, I know\n",
        "# the other approach is to use spacy\n",
        "# https://spacy.io/usage/models\n",
        "# we will dive into spacy later, but I think its important to keep building the intuition\n",
        "# before going into model-driven work\n",
        "\n",
        "# last, we can always add to the stoplist if we wanted to now that its a list abvoe\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p0N0yoOPFIT"
      },
      "source": [
        "# lets keep the corpus small, so use the original \n",
        "# but remove stopwords\n",
        "cv = CountVectorizer(stop_words=STOPWORDS)\n",
        "\n",
        "\n",
        "\n",
        "# 37 -> 30\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWtMOgqDQN6z"
      },
      "source": [
        "# and of course, we can see the vocab\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_kM4LexQ3iH"
      },
      "source": [
        "###################################### Max tokens\n",
        "## \n",
        "## this can be helpful if you want to restrict to the top N most frequent tokens\n",
        "## this restricts your space at the start\n",
        "## but the tradeoff is less common words, perhaps, could help with ML models\n",
        "##     -- the tokens/phrases are specific to he known label, and while rare, often occur for the label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNC6RrVdQ3mk"
      },
      "source": [
        "## we can use the article again, max with stopwords\n",
        "\n",
        "# cv = CountVectorizer(max_features=20, stop_words=STOPWORDS)\n",
        "# atokens = cv.fit_transform([atext])\n",
        "# cv.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s-1bSY2Q3pp"
      },
      "source": [
        "###################################### character tokens\n",
        "## \n",
        "## if you wanted, you can parse characters\n",
        "## a little out of scope, but highlighting the concept of tokenization can \n",
        "## take all sorts of forms!\n",
        "\n",
        "x = [\"Hello I can't\"]\n",
        "\n",
        "# charvec = CountVectorizer(analyzer='char', ngram_range=(1,1))\n",
        "# cv = charvec.fit_transform(x)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOQ0kaSWQ3ru"
      },
      "source": [
        "###################################### custom pattern\n",
        "## \n",
        "## Finally, if you really wanted to (or needed to), you can roll your own\n",
        "## tokenization\n",
        "## This is a little forward looking  .....\n",
        "## but highlights you all have the power to roll your own\n",
        "##\n",
        "## https://stackoverflow.com/questions/1576789/in-regex-what-does-w-mean\n",
        "##\n",
        "\n",
        "# alpha numeric plus a single quote/contraction\n",
        "PATTERN = \"[\\w']+\"\n",
        "\n",
        "cv = CountVectorizer(token_pattern=PATTERN)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue2kIPgnWeg-"
      },
      "source": [
        "###################################### Your Turn\n",
        "## \n",
        "## get the text from the two articles below\n",
        "## 1.  https://towardsdatascience.com/can-we-please-stop-using-word-clouds-eca2bbda7b9d\n",
        "## 2.  https://www.businessinsider.com/pie-charts-are-the-worst-2013-6\n",
        "##\n",
        "## create a bag of words representation of the two documents\n",
        "## keep the top 250 word tokens\n",
        "## remove stopwords\n",
        "## use tokens, bigrams (2) and trigrams(3)\n",
        "## TRICKY!  Put back into a dataframe if you can\n",
        "\n",
        "# a1 = Article(\"https://towardsdatascience.com/can-we-please-stop-using-word-clouds-eca2bbda7b9d\")\n",
        "# a1.download()\n",
        "# a1.parse()\n",
        "# a2 = Article(\"https://www.businessinsider.com/pie-charts-are-the-worst-2013-6\")\n",
        "# a2.download()\n",
        "# a2.parse()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL4NvOGjjzZh"
      },
      "source": [
        "## remember, jaccard is intersection over union, \n",
        "## instead of counts, we just said \"is this word present\"\n",
        "## value is proportion of elements that disagree\n",
        "\n",
        "## lets do a little more parsing before we start clustering!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQZWZX-r1UK7"
      },
      "source": [
        "############################################################\n",
        "########################################### Team Challenge\n",
        "############################################################\n",
        "\n",
        "## Get a first pass model built to predict if an SMS message is spam!\n",
        "## \n",
        "## review the slides at the end of this module\n",
        "## predict spam based on the message\n",
        "## objective =  based on accuracy\n",
        "## \n",
        "## only input is text, but you can derive features if you like\n",
        "## limited time, but how do you maximize your initial first pass (and the model?)\n",
        "## HINTS:\n",
        "##        start small, simple models\n",
        "##        iterate and see how you do against the leaderboard\n",
        "##        code above helps you with the core mechanics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM_7uwWSfWaE"
      },
      "source": [
        "###################################### NLTK parsing\n",
        "###################################### Quick highlight that there are pre-built tools!\n",
        "## \n",
        "## we may not have to reinvent the wheel!\n",
        "## NLTK has some built in tooling we can leverage!\n",
        "## and trust me, other toolkits have their own approaches too!\n",
        "\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer, WordPunctTokenizer, TweetTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQOuKnJogvkd"
      },
      "source": [
        "# we may also need to download a tool to help with (sentence) parsing amongst other tasks\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyDPDBJtfWb6"
      },
      "source": [
        "corpus = ['I want my MTV!', \"Can't I have it all for $5.00 @customerservice #help\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us-Enha7gjhc"
      },
      "source": [
        "## first thing: simple word tokenize\n",
        "\n",
        "# tokens = []\n",
        "# for doc in corpus:\n",
        "#   tokens.append(word_tokenize(doc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wY6fFScgz7U"
      },
      "source": [
        "## what do you notice?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5AKLCm2g6rQ"
      },
      "source": [
        "# lets dive deeper\n",
        "wp = WordPunctTokenizer()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# even more granular split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81M_SlMGg6wP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45duUyrLg6yc"
      },
      "source": [
        "# just like scikit, roll our own\n",
        "# basically split on whitespace by NOT (uppercase) selecting whitespace\n",
        "\n",
        "# note we have to add our pattern below\n",
        "regtok = RegexpTokenizer()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# strips out other bits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nxDJUJwhpiS"
      },
      "source": [
        "# finally, a tokenizer to help with twitter, and perhaps other social data\n",
        "social = TweetTokenizer()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# what do we have\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_N8xdZWibPz"
      },
      "source": [
        "###################################### Summary\n",
        "## \n",
        "## we have super powers via regex, but don't be afraid to look around\n",
        "## some decent tools in sklearn, but nltk has some custom utilities we can leverage\n",
        "##\n",
        "## We have options!  \n",
        "## we can try to parse with nltk and feed to sklearn\n",
        "## we can use the tooling in sklearn but might require we roll our own modifications\n",
        "##\n",
        "## but generally the flow is pre/tokenize -> bag of words of those tokens\n",
        "##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxno_Q1SxO6z"
      },
      "source": [
        "corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY5Vgb5UwtjM"
      },
      "source": [
        "############################### So the big question\n",
        "## how does this all fit together?\n",
        "\n",
        "# build a function to pull in the bits we want from NLTK, or whatever framework we want to use\n",
        "def tokenize(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# NOTE: lower case happens below, not above\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV6AkgwRxwF2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMPmuDKFmt4V"
      },
      "source": [
        "###################################### Next Up:  Beyond simple counts with TFIDF\n",
        "##\n",
        "## instead of count vectors (which you can use, and should try, in your modeling!)\n",
        "## we can try to de-prioritize common words \n",
        "## This surfaces words that may be less common, but nuanced and we want to prioritize those tokens\n",
        "## "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl5DA1o4z0ck"
      },
      "source": [
        "# the same data\n",
        "corpus = [\"Can't I have it all for $5.00 @customerservice #help\", \n",
        "          'I want my MTV!']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpirFnkVxM5n"
      },
      "source": [
        "# equivalent to CountVectorizer -> TfidfTransformer\n",
        "# basically if you want tfidf, do this, it saves a step\n",
        "# and you have the same options for parsing if you like\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suzLPb717CJc"
      },
      "source": [
        "## just to call out, being able to specify the pattern can be \n",
        "## really powerful for specific tasks and business needs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6R_Tt0BxMzX"
      },
      "source": [
        "# lets put this into a dataframe\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUME5JpYxMnc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-fChA66-F3n"
      },
      "source": [
        "# we could even heatmap this to help understand the intuition here\n",
        "\n",
        "# plt.figure(figsize=(4,6))\n",
        "# sns.heatmap(idf.T, xticklabels=True, yticklabels=True, cmap='Reds')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb8oMLOr726T"
      },
      "source": [
        "################### NOTE:\n",
        "## look at the weights generally, what do you see?\n",
        "## now focus in on the word in common, the token i\n",
        "##\n",
        "## we can see that when shared, there is the document effect\n",
        "\n",
        "# https://towardsdatascience.com/a-gentle-introduction-to-calculating-the-tf-idf-values-9e391f8a13e5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s3QKvd8_BvH"
      },
      "source": [
        "# lets break the intuition down\n",
        "\n",
        "# first the scores\n",
        "# ivals = idf[[\"i\"]]\n",
        "# print(ivals)\n",
        "\n",
        "## now just look at the statements, focus on i\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWbvkdda_Eg3"
      },
      "source": [
        "## ^^ above, you can see that i is only 1 of 4 words\n",
        "##    it highlights how its natrual to assume i, in that context, has a higher weight \n",
        "##    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlPIlPtR_E0u"
      },
      "source": [
        "## on more try to extend this\n",
        "\n",
        "# corpus2 = corpus.copy()\n",
        "# corpus2.extend([\"I want help\", \"halp\", \"python is fun\", \"Can I get help\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEfnkFAoCKGu"
      },
      "source": [
        "# refit - but simplify, just tokens (unigrams)\n",
        "\n",
        "# tfidf2 = TfidfVectorizer(token_pattern=\"[\\w']+\", ngram_range=(1,1))\n",
        "# tfidf2.fit(corpus2)\n",
        "# idf2 = tfidf2.transform(corpus2)\n",
        "# idf2 = pd.DataFrame(idf2.toarray(), columns=tfidf2.get_feature_names())\n",
        "# plt.figure(figsize=(4,6))\n",
        "# sns.heatmap(idf2.T, xticklabels=True, yticklabels=True, cmap=\"Reds\")\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqn1YoyuDNRg"
      },
      "source": [
        "## again could imagine that we might want to tighten up parsing of amounts\n",
        "## but this is starting to get us to think about NER later own this semester\n",
        "## for now, think of entity extraction as regex pattern searches\n",
        "\n",
        "\n",
        "## but why does this matter?\n",
        "##  We can think of tfidf as attempting to create a more informative feature space\n",
        "##  when we think about similiarty, or how we could reduce this space easily, \n",
        "##  its not hard to consider that DR techniques give us ways to compress but \n",
        "##  we lose the ability to describe the impact of a given token.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itiX37_JJZBP"
      },
      "source": [
        "# its easy to imagine this as a large space if we open up how we tokenize + ngrams\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x03DNd6fzgKq"
      },
      "source": [
        "# lets tsne the data and plot the docs\n",
        "# our data are so small that we dont need something like PCA in front\n",
        "\n",
        "# from sklearn.manifold import TSNE\n",
        "\n",
        "# tsne = TSNE()\n",
        "# t2 = tsne.fit_transform(idf2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7kNkRx7GY_k"
      },
      "source": [
        "# we just compressed the tfidf array to 2d\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp_8cGivmt8-"
      },
      "source": [
        "# lets plot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V1NrKFF2B9r"
      },
      "source": [
        "# now lets fit a kmeans to the dataframe\n",
        "# what falls out with k = 2?\n",
        "# OVERLY simplistic of course, but the hardest part was getting the feature vectors setup via tokenization\n",
        "# obviously we have some flexibility on how we tokenize, the ngrams, stopwords, max threshold, etc\n",
        "# so solutions can take different forms!\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXUqIBjTpe9U"
      },
      "source": [
        "## if you wanted to see a different approach, use the count vectors\n",
        "# cv = CountVectorizer(token_pattern=\"[\\w']+\", ngram_range=(1,1))\n",
        "# cv.fit(corpus2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-87PfJ6muCM"
      },
      "source": [
        "# idf_c = cv.transform(corpus2)\n",
        "# idf_c = pd.DataFrame(idf_c.toarray(), columns=cv.get_feature_names())\n",
        "# plt.figure(figsize=(4,6))\n",
        "# sns.heatmap(idf_c.T, xticklabels=True, yticklabels=True, cmap=\"Reds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC60NbXvO42O"
      },
      "source": [
        "###################################### SUMMARY\n",
        "## \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDJdLlHg1qKF"
      },
      "source": [
        "###################################### BREAKOUT Challenge\n",
        "##\n",
        "## Get the topics from big query\n",
        "## questrom:datasets.topics\n",
        "## parse the text into bag of words\n",
        "## (only the text, not the category) - your choice on tokenization and weighting/feature space\n",
        "## cluster the text\n",
        "## how many clusters do you have?\n",
        "## overlay the category on top of the clusters\n",
        "## if we didn't have the category, any evidence that  text processing and clustering would help\n",
        "## find patterns?  Are there documents that appear to be outliers?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF9CxtJipF8A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmchdVzMpGWw"
      },
      "source": [
        "###################################### whats next?\n",
        "##\n",
        "## sentiment analysis- the easy vs the good (in my opinion, of course)\n",
        "## review how/why this can work, and why sentiment is easy to do poorly\n",
        "## continue to see how text and machine learning fit very well together\n",
        "## spacy to get us thinking about parsing the entities, and gensim preview\n",
        "## "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}